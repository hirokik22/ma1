{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mandatory Assignment 1\n",
    "\n",
    "#### Part II: Convolutional Neural Networks\n",
    "\n",
    "***\n",
    "\n",
    "Please see the description of the assignment in the README file (section 2) <br>\n",
    "**Guide notebook**: [material/cnns_pytorch.ipynb](material/cnns_pytorch.ipynb)\n",
    "\n",
    "Table of contents:\n",
    "1. Activate GPU\n",
    "2. Load data\n",
    "3. Inspect data\n",
    "4. Convolutional neural network (**Where you will implement the CNN**)\n",
    "5. Training hyperparameters (**Where you will add training parameters**)\n",
    "6. Training\n",
    "7. Plot loss and accuracy\n",
    "8. Evaluate\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in relevant libraries, and alias where appropriate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision  # noqa\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F  # noqa\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, a bit of a hack in case your IDE wants to run the notebook from /`assignment/` and not the project root folder `/ma1`. We need the working directory to be `/ma1` for local imports to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory set to: /Users/hiroki/Desktop/AIML25/mas/ma1\n"
     ]
    }
   ],
   "source": [
    "# Ensure the working directory is set to the \"ma1\" folder.\n",
    "while Path.cwd().name != \"ma1\" and \"ma1\" in str(Path.cwd()):\n",
    "    os.chdir(\"..\")  # Move up one directory\n",
    "print(f\"Working directory set to: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are local files imported into this notebook. This is one of the advantages of not using an online notebook like Google Colab or stand-alone notebooks like Jupyter: We can import local files and use them in our code, thus making it easier to manage our code and create a more modular structure.\n",
    "\n",
    "In `/src`, we have the following files:\n",
    "- `utils.py`: Contains utility functions (e.g., setting our device to GPU if available)\n",
    "- `data.py`: Contains functions to load data, train/validation split, and data augmentation\n",
    "- `training.py`: Contains a single but very important function: Our training loop.\n",
    "- `evaluation.py`: Contains functions to evaluate our model and produce a classification report\n",
    "- `visualization.py`: Contains functions to visualize our data and model performance\n",
    "\n",
    "You are encouraged to look at these files to understand how they work. Particularly, the training and evaluation files are important to understand how we train our model and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local files\n",
    "from src.utils import get_device\n",
    "from src.data import load_torch_data, to_dataloader, train_val_split\n",
    "from src.training import fit\n",
    "from src.evaluation import evaluate\n",
    "from src.visualize import plot_training_history, plot_probabilities, show_cifar_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Activate GPU\n",
    "If available. Note that this is not necessary, but it will speed up your training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pytorch version (2.5.1) with backend = mps\n"
     ]
    }
   ],
   "source": [
    "device = get_device()  # will default to 'cpu' if gpu is not available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11ffb4290>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64  # number of samples per batch\n",
    "\n",
    "torch.manual_seed(42)  # Set a random seed to ensure reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:15<00:00, 11.1MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/cifar-10-python.tar.gz to data\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'len(train_val)=50000, len(test)=10000'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use transforms.compose method to reformat images for modeling,\n",
    "# and save to variable preprocessing_stepss for later use.\n",
    "# Think of this like sci-kit learn's pipeline method.\n",
    "preprocessing_steps = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((32,32)),             # Cifar-10 images are 32x32\n",
    "        transforms.RandomCrop(32, padding=4),   # Data augmentation step: Randomly crop the image to 32x32\n",
    "        transforms.RandomHorizontalFlip(),      # Data augmentation step: Randomly flip image horizontally\n",
    "        transforms.ToTensor(),                  # Convert the image to a pytorch tensor\n",
    "        transforms.Normalize(                   # Normalize the image (i.e. scale the image to have a mean of 0 and a standard deviation of 1)\n",
    "            mean=[0.4914, 0.4822, 0.4465],      # FYI: The mean of the CIFAR10 dataset for your convenience\n",
    "            std=[0.2023, 0.1994, 0.2010]        # FYI the standard deviation of the CIFAR10 dataset\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# load the training/validation data\n",
    "train_val = load_torch_data(\n",
    "    dataset=\"CIFAR10\",\n",
    "    root = 'data',                     # The root directory where the dataset will be stored\n",
    "    download = True,                   # If the dataset is not found at root, it will be downloaded\n",
    "    train = True,                      # The train dataset (as opposed to the test dataset)\n",
    "    transform = preprocessing_steps  # transformations to be applied to the dataset (see steps above)\n",
    ")\n",
    "\n",
    "# load the testing data\n",
    "test = load_torch_data(\n",
    "    dataset = \"CIFAR10\",\n",
    "    root = 'data',\n",
    "    download = True,\n",
    "    train = False,\n",
    "    transform = preprocessing_steps\n",
    ")\n",
    "\n",
    "f\"{len(train_val)=}, {len(test)=}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'len(val)=10000, len(train)=40000'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split training data in training and validation (just like train_test_split in sklearn)\n",
    "train, val = train_val_split(train_val, val_ratio=0.2, seed=42)\n",
    "\n",
    "f\"{len(val)=}, {len(train)=}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloders for easy batch loading during training\n",
    "train_loader = to_dataloader(train, batch_size = batch_size, shuffle = True)\n",
    "val_loader = to_dataloader(val, batch_size = batch_size, shuffle = False)\n",
    "test_loader = to_dataloader(test, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset CIFAR10\n",
       "     Number of datapoints: 50000\n",
       "     Root location: data\n",
       "     Split: Train\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)\n",
       "                RandomCrop(size=(32, 32), padding=4)\n",
       "                RandomHorizontalFlip(p=0.5)\n",
       "                ToTensor()\n",
       "                Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])\n",
       "            ),\n",
       " Dataset CIFAR10\n",
       "     Number of datapoints: 10000\n",
       "     Root location: data\n",
       "     Split: Test\n",
       "     StandardTransform\n",
       " Transform: Compose(\n",
       "                Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)\n",
       "                RandomCrop(size=(32, 32), padding=4)\n",
       "                RandomHorizontalFlip(p=0.5)\n",
       "                ToTensor()\n",
       "                Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.201])\n",
       "            ))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAGxCAYAAADLfglZAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAMBlJREFUeJzt3Xtw1GWe7/FPp9Pp3DoNMSSdQIhZbqJcZhVEGC/ArCkzpUeH8QyjVbNYbrk6glMUWu4iNWvW3SGWtVpOFSsz63oYXLVgt1YdZ8QLM0jQg+wCCwuLjgtjkKCEQIDc00l3P+ePWfsYufg8kPiQ5P2q6irS/eWb59e/X/cnv3Tn2wFjjBEAAB5k+F4AAGD4IoQAAN4QQgAAbwghAIA3hBAAwBtCCADgDSEEAPCGEAIAeEMIAQC8IYQwJP3iF79QIBDQjh07+qVfIBDQkiVL+qXXF3vW1NT0a8/ztXLlSr366qu+l4FhiBACQAjBG0IIAOANIYRhq7u7Ww8++KC+8Y1vKBqNqrCwULNnz9Yvf/nLs/6fn//855o4caLC4bAuv/xyrVu37rSaxsZG3XvvvRozZoyysrJUWVmpv/7rv1YikejX9X/66af68z//c5WXlysrK0tlZWW6/fbbdfToUaftCwQC6ujo0Nq1axUIBBQIBDR37tx+XStwNpm+FwD4Eo/HdeLECT300EMaPXq0enp69Jvf/EYLFizQmjVr9Kd/+qd96l977TW98847euyxx5SXl6dnnnlGd9xxhzIzM3X77bdL+kMAXX311crIyNBf/dVfady4cXr//ff1t3/7tzp48KDWrFlzzjVdeumlkqSDBw+es+7TTz/VzJkz1dvbq0ceeUTTpk1Tc3Oz3nrrLZ08eVIlJSXW2/f+++9r/vz5mjdvnn784x9LkgoKCs7jHgXOgwGGoDVr1hhJZvv27db/J5FImN7eXvNnf/Zn5o//+I/73CbJ5OTkmMbGxj71l112mRk/fnz6unvvvdfk5+ebTz75pM///7u/+zsjyezbt69Pz0cffbRP3bhx48y4ceO+cq133323CYVC5oMPPuiX7cvLyzOLFi2y7gX0F34dh2HtX/7lX/TNb35T+fn5yszMVCgU0nPPPacPP/zwtNpvfetbKikpSX8dDAa1cOFCHThwQIcPH5Yk/frXv9a8efNUVlamRCKRvlRXV0uS6urqzrmeAwcO6MCBA1+57jfeeEPz5s3T5MmT+237AB8IIQxbL7/8sr73ve9p9OjReuGFF/T+++9r+/btuvvuu9Xd3X1afSwWO+t1zc3NkqSjR4/qV7/6lUKhUJ/LFVdcIUk6fvx4v6z92LFjGjNmzDlrXLcP8IHXhDBsvfDCC6qsrNT69esVCATS18fj8TPWNzY2nvW6Sy65RJJUVFSkadOm6Sc/+ckZe5SVlV3osiVJo0aNSp99nY3r9gE+EEIYtgKBgLKysvo8QTc2Np713XG//e1vdfTo0fSv5JLJpNavX69x48alz0puvvlmbdiwQePGjdPIkSMHbO3V1dX6p3/6J3300UeaNGnSGWtcti8cDqurq2vA1gucDSGEIW3Tpk1nfKfZt7/9bd188816+eWXdf/99+v2229XQ0OD/uZv/kalpaXav3//af+nqKhI8+fP149//OP0u+N+97vf9Xmb9mOPPaaNGzdqzpw5+tGPfqRJkyapu7tbBw8e1IYNG/Szn/3snL9GGz9+vCR95etCjz32mN544w1df/31euSRRzR16lSdOnVKb775ppYtW6bLLrvMafumTp2qzZs361e/+pVKS0sViUTOGm5Av/L9zghgIHz+7rizXerr640xxjz++OPm0ksvNeFw2EyePNk8++yz5tFHHzVffmhIMosXLzbPPPOMGTdunAmFQuayyy4zL7744mnf+9ixY+ZHP/qRqaysNKFQyBQWFpqrrrrKrFixwrS3t/fp+eV3x1VUVJiKigqrbWxoaDB33323icViJhQKmbKyMvO9733PHD16NF1ju327d+823/zmN01ubq6RZG644QarNQAXKmCMMX7iDwAw3PHuOACAN4QQAMAbQggA4A0hBADwhhACAHhDCAEAvLno/lg1lUrps88+UyQS6fOX3gCAwcEYo7a2NpWVlSkj49znOhddCH322WcqLy/3vQwAwAVqaGj4ykG7F92v4yKRiO8lAAD6gc3z+YCF0DPPPKPKykplZ2frqquu0rvvvmv1//gVHAAMDTbP5wMSQuvXr9fSpUu1YsUK7dq1S9ddd52qq6t16NChgfh2AIBBakBmx82aNUtXXnmlVq9enb5u8uTJuu2221RbW9unNh6P9/l8k9bWVl4TAoAhoKWlRQUFBees6fczoZ6eHu3cuVNVVVV9rq+qqtLWrVtPq6+trVU0Gk1fCCAAGD76PYSOHz+uZDKZ/uCvz5WUlJzxkymXL1+ulpaW9KWhoaG/lwQAuEgN2Fu0v/yClDHmjC9ShcNhhcPhgVoGAOAi1u9nQkVFRQoGg6ed9TQ1NZ12dgQAGN76PYSysrJ01VVXaePGjX2u//wjjwEA+NyA/Dpu2bJl+sEPfqAZM2Zo9uzZ+od/+AcdOnRI991330B8OwDAIDUgIbRw4UI1Nzfrscce05EjRzRlyhRt2LBBFRUVA/HtAACD1ID8ndCFaG1tVTQa9b0MAMAF8vJ3QgAA2CKEAADeEEIAAG8IIQCAN4QQAMAbQggA4A0hBADwhhACAHhDCAEAvCGEAADeEEIAAG8IIQCAN4QQAMAbQggA4A0hBADwhhACAHhDCAEAvCGEAADeEEIAAG8IIQCAN4QQAMCbTN8L+LplZgSc6lMpl+qgU2+nnwEcf1wwSlrX5ua4HQYTxl/qVF9QkGtd293V6dS7ufmkde2RRvtaSeqK2+/8gNyOK8k41ttz/sky4LB2x8ePHB4/KeN2nwSD9vWhLLfHZjic5VSfnRMakFpJyg7arz2U4badwaDDYz9ov++TyZT2/NcnVrWcCQEAvCGEAADeEEIAAG8IIQCAN4QQAMAbQggA4A0hBADwhhACAHhDCAEAvCGEAADeDLuxPU4jSiQpw340iEm5jmKxn2kSdF63fX3Z6FKn1peOHu1Un5NjPwLlxInjTr272jusa0cW5Dn1Nifte8cT9mOSpIEc2iOlHEcIufwkmko6zbGScdhS49ZaLvdiIOHWORDoday3r81wfCxnZtvXZzqO7Qk4PE9kOIztkcMxyJkQAMAbQggA4A0hBADwhhACAHhDCAEAvCGEAADeEEIAAG8IIQCAN4QQAMAbQggA4A0hBADwZvjNjnOeq2Vfn3KcCGac1uK27mCmfX1+Xo5T78Jo1Km+oCDfujYr0+3noo7OTuvarm63eWB5cfuBY4mOLqfeiaTbsRJw3P8uUsZ+LamU24A3l9lkAccfiV0eP4let/s7kehxqu/ttT9WenvcjsPurpB1bXbY7Sk9nG3fO5xlP5cu6TBjkDMhAIA3/R5CNTU1CgQCfS6xWKy/vw0AYAgYkF/HXXHFFfrNb36T/joYdBsvDgAYHgYkhDIzMzn7AQB8pQF5TWj//v0qKytTZWWlvv/97+vjjz8+a208Hldra2ufCwBgeOj3EJo1a5aef/55vfXWW3r22WfV2NioOXPmqLm5+Yz1tbW1ikaj6Ut5eXl/LwkAcJHq9xCqrq7Wd7/7XU2dOlV/8id/otdff12StHbt2jPWL1++XC0tLelLQ0NDfy8JAHCRGvC/E8rLy9PUqVO1f//+M94eDocVDocHehkAgIvQgP+dUDwe14cffqjS0tKB/lYAgEGm30PooYceUl1dnerr6/Vv//Zvuv3229Xa2qpFixb197cCAAxy/f7ruMOHD+uOO+7Q8ePHNWrUKF1zzTXatm2bKioq+vtbnR+36R0DKuj0I4DbuJTc7Gzr2nDY7e+4CiL2Y3gkaWx5mXVtXsR+3ZLUk0pa13Z2x516d3Z2W9eG4m73YSJpv25X7of4wD0oTMq+t8uIH+d1uNY7jlXqcdifiV63x3I8bt+7p9vtKT2z0/4xkZ1l3zvlsN/7PYTWrVvX3y0BAEMUs+MAAN4QQgAAbwghAIA3hBAAwBtCCADgDSEEAPCGEAIAeEMIAQC8IYQAAN4QQgAAbwb8oxwuOq7jqQZ01px985TjQiLRPOvavFy3eW3xuNsMtva2Duva7m77eW2S1N5+yro2M+S287Oy7H9GC4fdHkrBTLePL0kmE9a13d09Tr1djqyA84+t9ve5cRupNsAG7ufzlON2phL2a+lKuDXPStnvH5fpiA6j4zgTAgD4QwgBALwhhAAA3hBCAABvCCEAgDeEEADAG0IIAOANIQQA8IYQAgB4QwgBALwZfmN7jNv4G5chGCnXSHeZgWKSTq2zs7Ksa/Oy3cb2BOK9TvUjcnOta1NJt5FAJmE/5ieU6TbSJDvP/uHR6TjKKCPodrBkBu3vQ+N4jCcd5si49nYrdxur5DLKyrjMkdH5TOuy/x+BDLd977Kd4Uy3+7Ag1/55YmSe/TGYTKV0oqXdqpYzIQCAN4QQAMAbQggA4A0hBADwhhACAHhDCAEAvCGEAADeEEIAAG8IIQCAN4QQAMAbQggA4M3wmx3nOhUq5TCLKWPg5lMFHX9ecJmVlZ3pdhhkJNzmpIUD9nPvspRwW0vKvrdJOO77pMO+dxtLp1TKbf5ewtivJZDhNj8sGLCvTzrehQGXeXBuy1bAYS0Od58k99l+eQ7zEV3Fe+znI+ZnB516jyzIsa4tKrDfxkQyJR22q+VMCADgDSEEAPCGEAIAeEMIAQC8IYQAAN4QQgAAbwghAIA3hBAAwBtCCADgDSEEAPCGEAIAeDMMZ8e5DpEawN4O0+MyMt1mQmWG7HdtRsBt8Fmm6XGqr//vD6xrG5sbnXp3tJyyrg0kQ069g6ks+95y2z8ybjPyenvt5/X19rjtn+wc+/lhrnMGe3od1hJ0Ow5TSft6x9F+yg67HSuhsP0TheuzRDBofxxGcsJOvSO59vVB4zCn0djf45wJAQC8cQ6hLVu26JZbblFZWZkCgYBeffXVPrcbY1RTU6OysjLl5ORo7ty52rdvX3+tFwAwhDiHUEdHh6ZPn65Vq1ad8fYnnnhCTz31lFatWqXt27crFovpxhtvVFtb2wUvFgAwtDi/JlRdXa3q6uoz3maM0dNPP60VK1ZowYIFkqS1a9eqpKREL730ku69994LWy0AYEjp19eE6uvr1djYqKqqqvR14XBYN9xwg7Zu3XrG/xOPx9Xa2trnAgAYHvo1hBob//DOppKSkj7Xl5SUpG/7straWkWj0fSlvLy8P5cEALiIDci74wJf+rhgY8xp131u+fLlamlpSV8aGhoGYkkAgItQv/6dUCwWk/SHM6LS0tL09U1NTaedHX0uHA4rHHZ7bzsAYGjo1zOhyspKxWIxbdy4MX1dT0+P6urqNGfOnP78VgCAIcD5TKi9vV0HDhxIf11fX6/du3ersLBQY8eO1dKlS7Vy5UpNmDBBEyZM0MqVK5Wbm6s777yzXxcOABj8nENox44dmjdvXvrrZcuWSZIWLVqkX/ziF3r44YfV1dWl+++/XydPntSsWbP09ttvKxKJ9N+qv0YZLieLjvM4Ug6tM7PcTlpHRAusa1tOnHLqrYT9CBlJGlkUta7t6ely6l3osJ2tHfZjRySpLcN+LUGHEUySFHAcJJOdZT9GJj9vhFPvzHC2dW28u9epd3eH/X2YnWs/nkaScqL2x1Wb47tuUym37Uw4HFrGYdyQJOVl279cUZDv9tKGy3Gb47COXodtdA6huXPnypizLzwQCKimpkY1NTWurQEAwwyz4wAA3hBCAABvCCEAgDeEEADAG0IIAOANIQQA8IYQAgB4QwgBALwhhAAA3hBCAABv+vWjHAYHxwFvLgYw0o1xmzeVSjnUB+znkklSyGFemySNrvwj69qYcZvZdexks3VtXrvbXLrgWT4D60xCDrPdJOlku9v8vZTL2LsMtwOxN9FjX9trXytJOseIry9zffgU5OfaFyfdjquubrdjJeAwK831sZybbT9Tb4TLfSIp0NtpXRvJs58x2OswTI8zIQCAN4QQAMAbQggA4A0hBADwhhACAHhDCAEAvCGEAADeEEIAAG8IIQCAN4QQAMCbYTi25+LhMkDIZQyGJNV//Il1becl+U69S6ZPdqo/0WU/GkRJt7EwJpCwrs3Ptx9/IkmJzrB1bVZ2jlPvZMYpp/rPDh+1L85wGyFUOjZmXdvR7rAvJbUG7Y/yTMcfiTvbWqxrM2Q/PkiSQoGgU31Xd7d1bX6e27Hicr8kE27joJLxDuvaQMJ+JFCAsT0AgMGAEAIAeEMIAQC8IYQAAN4QQgAAbwghAIA3hBAAwBtCCADgDSEEAPCGEAIAeEMIAQC8GXaz4wJOE9ukgENOJ+U23y2Qab+WvHC2U+9k0n4t3cmUU+/jPW1O9c2HG61rsx3vw8KCqHVtbnaeU++iSwqta0uyIk69M4Ju+7OzxX5mW69TZyk3bD8jL9njNtsvu9j+Phw50r5Wkup/X29d293ttu68XLf5e9ER9nPVssJuMwwL8uzXMqLA7bg60Wg/f68r3mVd2+vwnMKZEADAG0IIAOANIQQA8IYQAgB4QwgBALwhhAAA3hBCAABvCCEAgDeEEADAG0IIAODNsBvbY2Sc6gNB+/qQwxgeScrJsR+xMaZ0lFPvkMOPF6WxIqfeWQG3sSOBzIR17SUj8p16RyMF1rXxTrfRLcoOWpcmE27Dcgoj9mNeJOlP5l1vXfv+jh1OvZM9ceva/Fz7+0SSRo6wH8Vz9OgJp94y9sdVxVjHx0/AbX8GHJ5WQpluI4GygvbPK7lZbo/NwnGXWte6bGNPb1LSYatazoQAAN4QQgAAb5xDaMuWLbrllltUVlamQCCgV199tc/td911lwKBQJ/LNddc01/rBQAMIc4h1NHRoenTp2vVqlVnrbnpppt05MiR9GXDhg0XtEgAwNDk/MaE6upqVVdXn7MmHA4rFoud96IAAMPDgLwmtHnzZhUXF2vixIm655571NTUdNbaeDyu1tbWPhcAwPDQ7yFUXV2tF198UZs2bdKTTz6p7du3a/78+YrHz/w20NraWkWj0fSlvLy8v5cEALhI9fvfCS1cuDD97ylTpmjGjBmqqKjQ66+/rgULFpxWv3z5ci1btiz9dWtrK0EEAMPEgP+xamlpqSoqKrR///4z3h4OhxV2+Ix7AMDQMeB/J9Tc3KyGhgaVlpYO9LcCAAwyzmdC7e3tOnDgQPrr+vp67d69W4WFhSosLFRNTY2++93vqrS0VAcPHtQjjzyioqIifec73+nXhQMABj/nENqxY4fmzZuX/vrz13MWLVqk1atXa+/evXr++ed16tQplZaWat68eVq/fr0ikUj/rfpCOMyCk6RQ2P5ksaDAbR5Yacx+ntXIfPs5c5KUHbbftZE8t94nm0861ReXjLSuzcuPOvVuaWmxrs3MznPqraD9jK8P9vynU+tpk6c51Y+fNM66dueuXU69i0baz9/LznWbj5hMpqxre7q6nHpPv+KPrGsj+W7z2kIp+3VLUryz3bq2s7PDqXcgZf9YDvQmnXpXThhvXxy0X0d3vFfSHqta5xCaO3eujDn7E/lbb73l2hIAMEwxOw4A4A0hBADwhhACAHhDCAEAvCGEAADeEEIAAG8IIQCAN4QQAMAbQggA4A0hBADwZsA/yuFik5Pn9rERubn29SNGus2OKxllP8tsRG6OU++8PPtZfR3tnU69w1luc7gqyyusa3PDbrPJOk+dsq7NznLb94Gw/Uy9zGDQqffE8WOd6nuT9nPVMgJu88Nysux/Fs0MuP3c2nzshHXttMkTnHpPn2o/92z3zp1OvU2m23FYNKrQuvbUKbf5lYlO+/3Z3ek2f2/nf/6XdW1xmf3juKc3YV3LmRAAwBtCCADgDSEEAPCGEAIAeEMIAQC8IYQAAN4QQgAAbwghAIA3hBAAwBtCCADgzbAb25MfKXCqz86zz+nCEW6jdbJDKevaYMB+DIYk5edkWdfGOzucekdy3cYTjS0rsa6NRtxG65i4/ZiS7Hy3fX+std26Nhqxv78laeQItzE/O//rgHWt48QZuazkxNEWp969XfbH+LVXT3XqnZNr//TV1tbm1HtE4Uin+oKimHVtc0ePU+9klv1xmEy59W5us3/sn6j/xLo2kbQfNcSZEADAG0IIAOANIQQA8IYQAgB4QwgBALwhhAAA3hBCAABvCCEAgDeEEADAG0IIAOANIQQA8GbYzY4LBN0Ga4XDIevagki2U++MVK91bctJ+xlpkpQVtF9LV3unU+9QwDjVtxw/Zl3b0+L2c1HnyVPWtflht7l0Yy+xnx+WM/lSp96BgNt9HnaYM3jd7Kucend22h9boQz7x4MkZZfa78/RRW6zFz9tarKuTabsZ5lJUtmYsU713b32++dUl9u+zw/Z34em1+35rSNuP2suw9jPR0wm7e8PzoQAAN4QQgAAbwghAIA3hBAAwBtCCADgDSEEAPCGEAIAeEMIAQC8IYQAAN4QQgAAb4bd2J5QptvImawM+5zu6bQfwyNJmeGgdW13j/0YDEn64MCn9uvIsF+HJFVfe7lT/eiiAuva/ft+59Q7rG7r2pNH/tup97jxE61rp8+92ql3S0e7U31FacK6NjOU57aW1hbr2smTip16J7rtR9SEkm6Pn9bmk9a1pcWlTr0zg24jng78/kP7tRTmO/UekZ9rXXv4k6NOvU2vw9ieoP3+MYztAQAMBoQQAMAbpxCqra3VzJkzFYlEVFxcrNtuu00fffRRnxpjjGpqalRWVqacnBzNnTtX+/bt69dFAwCGBqcQqqur0+LFi7Vt2zZt3LhRiURCVVVV6ujoSNc88cQTeuqpp7Rq1Spt375dsVhMN954o9ra2vp98QCAwc3pjQlvvvlmn6/XrFmj4uJi7dy5U9dff72MMXr66ae1YsUKLViwQJK0du1alZSU6KWXXtK99957Ws94PK54PJ7+urW19Xy2AwAwCF3Qa0ItLX94V01hYaEkqb6+Xo2NjaqqqkrXhMNh3XDDDdq6desZe9TW1ioajaYv5eXlF7IkAMAgct4hZIzRsmXLdO2112rKlCmSpMbGRklSSUlJn9qSkpL0bV+2fPlytbS0pC8NDQ3nuyQAwCBz3n8ntGTJEu3Zs0fvvffeabcFAn0/YtYYc9p1nwuHwwo7fuwyAGBoOK8zoQceeECvvfaa3nnnHY0ZMyZ9fSwWk6TTznqamppOOzsCAMAphIwxWrJkiV5++WVt2rRJlZWVfW6vrKxULBbTxo0b09f19PSorq5Oc+bM6Z8VAwCGDKdfxy1evFgvvfSSfvnLXyoSiaTPeKLRqHJychQIBLR06VKtXLlSEyZM0IQJE7Ry5Url5ubqzjvvHJANAAAMXk4htHr1aknS3Llz+1y/Zs0a3XXXXZKkhx9+WF1dXbr//vt18uRJzZo1S2+//bYikUi/LPhChYIhp/pEj/3MruYT8a8u+oJUfrZ1bVbQbb5brPgS69rcPPt1SFJOgf0sK0kqq4hZ17a1HXPqXRjNsq4NZ7v99jkcsp/xFcxwe3m1uMj+PpGkTz+xv18iuW7HePEo+3ekZmS4zV6Md9k/JkrLLnXq3ZmyX8vIUW4vBxw/1vHVRV9wxR/ZzxkcP77MqfeRI4etaxs+PuLUO9NhNmZm0OUYt58d5/TIMeard3ogEFBNTY1qampcWgMAhiFmxwEAvCGEAADeEEIAAG8IIQCAN4QQAMAbQggA4A0hBADwhhACAHhDCAEAvDnvj3IYrDIcx6t88VNfv0pv0m1sTzhgP3Ykf0SBU++iQvtRPF0O2yhJ727b5VSfGbQfrRPNdRvvVH/oE+vastJSp94f//6/rWvLx3Y69b7yyilO9VmZ9mObgqbXqXeGsR/z09NpP8ZKkiorx1vX5jru+66P2qxrO065jYNqPnLmzz87m1TizB9VcyZNmV1OvTvi3da1vSm3/TNy5Ajr2mCW/RirRCJpXcuZEADAG0IIAOANIQQA8IYQAgB4QwgBALwhhAAA3hBCAABvCCEAgDeEEADAG0IIAOANIQQA8GbYzY7Ly8t1qs/MsJ+XlJ3pNrNr0tjR1rUll4x06t3RcdK6NhFvd+qdTNnPgpOkXf/xkXXt/G9e7dR7+jfmWdceOvSpU+8Pfvcf1rXxuNu+n/BHZU71ZcVF1rUdHR1OvXPD9k8DsViJU+8Ch5mH/7zul069YyX2x+HEccVuvUe6PU80HDpqXRuU26zGkVH756DeHrfjsKWlx7p2xEj7eZQpZscBAAYDQggA4A0hBADwhhACAHhDCAEAvCGEAADeEEIAAG8IIQCAN4QQAMAbQggA4M2wG9uTSnU51efk2I+qGJFtXytJvV3d1rXx9k6n3u2n7Mf2ZGeHnHqPKnQbadLdbj9G5v1/3+7U+7b/tcC6tmT0OKfeXebfrWt/X3/EqXdjQ6NT/YhC+/E3ne2nnHqXlFxiXTsyEnHqve1d+/3527ftxyRJ0hVTR1jXXvnH5U6921tSTvWhsP39cu3865167//499a1IwvynHofPdZmXdvW2mpdm0ja33+cCQEAvCGEAADeEEIAAG8IIQCAN4QQAMAbQggA4A0hBADwhhACAHhDCAEAvCGEAADeEEIAAG8CxhjjexFf1Nraqmg0OmD9J18Rc6oPZdjfPWWF9jO4JKn5iP28sdGFI5x6Txg/2ro2ZXqdejc0HXeqN4GAdW04nOPUuyBvpHXt73//qVPvjw4ctq6dGHM7Zr993VSn+opL7Y+tZMJ+JqEkZecWWdd+diLh1Ps3dfaz48aWus32mzbdfh6ckdvMyN6E/TErSSNHlVnXnmy1n6UoSTu2v2dda4zbzLvOpP28y8OfHbWuTSRTend3vVpaWlRQcO65h5wJAQC8cQqh2tpazZw5U5FIRMXFxbrtttv00Ucf9am56667FAgE+lyuueaafl00AGBocAqhuro6LV68WNu2bdPGjRuVSCRUVVWljo6+p5c33XSTjhw5kr5s2LChXxcNABganD5P6M033+zz9Zo1a1RcXKydO3fq+uv//2dkhMNhxWJur70AAIafC3pNqKWlRZJUWFjY5/rNmzeruLhYEydO1D333KOmpqaz9ojH42ptbe1zAQAMD+cdQsYYLVu2TNdee62mTJmSvr66ulovvviiNm3apCeffFLbt2/X/PnzFY/Hz9intrZW0Wg0fSkvd/sERADA4HXeH++9ZMkS7dmzR++91/ftgwsXLkz/e8qUKZoxY4YqKir0+uuva8GC0z+Kefny5Vq2bFn669bWVoIIAIaJ8wqhBx54QK+99pq2bNmiMWPGnLO2tLRUFRUV2r9//xlvD4fDCofD57MMAMAg5xRCxhg98MADeuWVV7R582ZVVlZ+5f9pbm5WQ0ODSktLz3uRAIChyek1ocWLF+uFF17QSy+9pEgkosbGRjU2Nqqr6w9/jdze3q6HHnpI77//vg4ePKjNmzfrlltuUVFRkb7zne8MyAYAAAYvpzOh1atXS5Lmzp3b5/o1a9borrvuUjAY1N69e/X888/r1KlTKi0t1bx587R+/XpFIpF+WzQAYGhw/nXcueTk5Oitt966oAUNtIwM+1lJktTV3WZd296TdOodvcR+7lki0enUO1M91rUjI3lOvQNy+xswkxW0ri0YaX+fSJIcZnxlJNz2TzhlP4MtFHebqZbqdtufeUH7fZTv+Brrvt81WNdu3GNfK0m7Dhyyri3Iy3fqPebSb1rXdhm32XEfnOU17LP59y111rVNn55w6p1K2h8rZaOLHXvbP356HB4/iaT9DDtmxwEAvCGEAADeEEIAAG8IIQCAN4QQAMAbQggA4A0hBADwhhACAHhDCAEAvCGEAADenPfnCQ1WiR77USySFO+wH5nRldPu1PvyKZOsa9ub7MefSFLzsWPWtdGw/VgdSbp+xpSvLvqCzOws69pPDrltZ0aW/SE8babbuo+UFFnXfnLgoFPvEfkFTvWVoyusa5NdbiOEWk4ctK7tOnXEqfclI+xrPzh4wKn3z1/4Z+valk63MUmdnWf+EM6zGZGbY12blxdy6l2QZT/OKDPV69Q71WV/v+QF7UfxJMTYHgDAIEAIAQC8IYQAAN4QQgAAbwghAIA3hBAAwBtCCADgDSEEAPCGEAIAeEMIAQC8IYQAAN4Mu9lxeTn2M54kKSuQtK5tPXnSqXd7m/2suWlTv+HUu/vUUevaU20tTr1bO9zm78Ui9nPSIrlRp94nT7Ra15aU2c9fk6QrZ863rn3j5V879f7sk4+c6v97r/3+PN58wql3Z8JY1867boZT7+wR9o+3w81usxdPttvPyMsMuT3uc1L2x5UkRfPt58Hl5tvPVZOkrID9uUJPj9vcwEi+/f1iMgPWtb0J++dNzoQAAN4QQgAAbwghAIA3hBAAwBtCCADgDSEEAPCGEAIAeEMIAQC8IYQAAN4QQgAAb4bd2J7MoP14DUnKzrMfOZNKumX6Z0earWuvmznTqfeM6d+wru2Ndzn1DueOcKrPj+RZ15rQSKfe0WL7ESjlV1zj1LusYpJ17eyE/TZK0uv/51mn+v+77WPr2tziUU69Z1R9x7r20ksvcer96YHd1rUZqU+ceo/It6/tNm6PzZOXZDnVZ4bta3uSjuOJWu3HZHUr6NT7WMtx69p4hv3YnkTC/nHJmRAAwBtCCADgDSEEAPCGEAIAeEMIAQC8IYQAAN4QQgAAbwghAIA3hBAAwBtCCADgDSEEAPBm2M2O64gnnOpHFEStaydeMcGpd5bDmKejbW7z3UqNffOiophT71DA7bDp7u60rs0Muc3sqigvt66NFLttZ2cwx7p23MxrnXpfebDJqf5Ifb117fzv/W+n3qMnjrWuPXl4n1Pv33+4x7o22ePUWgob69JRo91mEhZmux0rR07az4Pr7Ch06t2R3WZd29JtP2dOkuIjIta1yYD97LhkT0LSQatazoQAAN44hdDq1as1bdo0FRQUqKCgQLNnz9Ybb7yRvt0Yo5qaGpWVlSknJ0dz587Vvn1uPzkBAIYPpxAaM2aMHn/8ce3YsUM7duzQ/Pnzdeutt6aD5oknntBTTz2lVatWafv27YrFYrrxxhvV1mZ/OgkAGD6cQuiWW27Rt7/9bU2cOFETJ07UT37yE+Xn52vbtm0yxujpp5/WihUrtGDBAk2ZMkVr165VZ2enXnrppYFaPwBgEDvv14SSyaTWrVunjo4OzZ49W/X19WpsbFRVVVW6JhwO64YbbtDWrVvP2icej6u1tbXPBQAwPDiH0N69e5Wfn69wOKz77rtPr7zyii6//HI1NjZKkkpKSvrUl5SUpG87k9raWkWj0fSl3OHdTgCAwc05hCZNmqTdu3dr27Zt+uEPf6hFixbpgw8+SN8e+NLb+Iwxp133RcuXL1dLS0v60tDQ4LokAMAg5fx3QllZWRo/frwkacaMGdq+fbt++tOf6i/+4i8kSY2NjSotLU3XNzU1nXZ29EXhcFjhsMMHtAMAhowL/jshY4zi8bgqKysVi8W0cePG9G09PT2qq6vTnDlzLvTbAACGIKczoUceeUTV1dUqLy9XW1ub1q1bp82bN+vNN99UIBDQ0qVLtXLlSk2YMEETJkzQypUrlZubqzvvvHOg1g8AGMScQujo0aP6wQ9+oCNHjigajWratGl68803deONN0qSHn74YXV1den+++/XyZMnNWvWLL399tuKROxHQwy0seVlA9Y7mXKbO9Iet6//zZYtTr0PH/nUunbeNTOcek8uH+1Un5llf8Ld3eU2diQnkLKuDSbdRjaleu3re+2XIUkafcVlTvWTr77Gujb2P78utxVP2Y9V6k65/fKkaPQ469qsvBFOvQNR+6evYwm3v1Xc13jYqf5gZ4d1bXtGtlPvroj9+KjePLexVzL2o3h6HZ6vkg7j0ZxC6Lnnnjvn7YFAQDU1NaqpqXFpCwAYppgdBwDwhhACAHhDCAEAvCGEAADeEEIAAG8IIQCAN4QQAMAbQggA4A0hBADwxnmK9kAzxgxo/16HUSzu3DI96bCWRMJtLkxPj33vzq64U+/2zi6nepO0H/cRdxzb09ZhP3Imt63dqXcqaT/qxXH3qL3DfsyLJAWM/UM11/GDIRPG/j5sa3dbd0en/f7scDwOA6GkdW1n0q13vLvXqb43bl/fGwg69U4kHJ4nUvb3iSQlHMb2uIziSf7P84/N83nADPSzvqPDhw/zwXYAMAQ0NDRozJgx56y56EIolUrps88+UyQS6fNheK2trSovL1dDQ4MKCgo8rnBgsZ1Dx3DYRontHGr6YzuNMWpra1NZWZkyMs79G6KL7tdxGRkZ50zOgoKCIX0AfI7tHDqGwzZKbOdQc6HbGY1Grep4YwIAwBtCCADgzaAJoXA4rEcffVThcNj3UgYU2zl0DIdtlNjOoebr3s6L7o0JAIDhY9CcCQEAhh5CCADgDSEEAPCGEAIAeEMIAQC8GTQh9Mwzz6iyslLZ2dm66qqr9O677/peUr+qqalRIBDoc4nFYr6XdUG2bNmiW265RWVlZQoEAnr11Vf73G6MUU1NjcrKypSTk6O5c+dq3759fhZ7Ab5qO++6667T9u0111zjZ7Hnqba2VjNnzlQkElFxcbFuu+02ffTRR31qhsL+tNnOobA/V69erWnTpqWnIsyePVtvvPFG+vavc18OihBav369li5dqhUrVmjXrl267rrrVF1drUOHDvleWr+64oordOTIkfRl7969vpd0QTo6OjR9+nStWrXqjLc/8cQTeuqpp7Rq1Spt375dsVhMN954o9ra7KdXXwy+ajsl6aabbuqzbzds2PA1rvDC1dXVafHixdq2bZs2btyoRCKhqqoqdXxhGvhQ2J822ykN/v05ZswYPf7449qxY4d27Nih+fPn69Zbb00Hzde6L80gcPXVV5v77ruvz3WXXXaZ+cu//EtPK+p/jz76qJk+fbrvZQwYSeaVV15Jf51KpUwsFjOPP/54+rru7m4TjUbNz372Mw8r7B9f3k5jjFm0aJG59dZbvaxnoDQ1NRlJpq6uzhgzdPfnl7fTmKG5P40xZuTIkeYf//Efv/Z9edGfCfX09Gjnzp2qqqrqc31VVZW2bt3qaVUDY//+/SorK1NlZaW+//3v6+OPP/a9pAFTX1+vxsbGPvs1HA7rhhtuGHL7VZI2b96s4uJiTZw4Uffcc4+ampp8L+mCtLS0SJIKCwslDd39+eXt/NxQ2p/JZFLr1q1TR0eHZs+e/bXvy4s+hI4fP65kMqmSkpI+15eUlKixsdHTqvrfrFmz9Pzzz+utt97Ss88+q8bGRs2ZM0fNzc2+lzYgPt93Q32/SlJ1dbVefPFFbdq0SU8++aS2b9+u+fPnKx53+6C1i4UxRsuWLdO1116rKVOmSBqa+/NM2ykNnf25d+9e5efnKxwO67777tMrr7yiyy+//GvflxfdRzmczRc/W0j6wwHy5esGs+rq6vS/p06dqtmzZ2vcuHFau3atli1b5nFlA2uo71dJWrhwYfrfU6ZM0YwZM1RRUaHXX39dCxYs8Liy87NkyRLt2bNH77333mm3DaX9ebbtHCr7c9KkSdq9e7dOnTqlf/3Xf9WiRYtUV1eXvv3r2pcX/ZlQUVGRgsHgaQnc1NR0WlIPJXl5eZo6dar279/veykD4vN3/g23/SpJpaWlqqioGJT79oEHHtBrr72md955p8/nfg21/Xm27TyTwbo/s7KyNH78eM2YMUO1tbWaPn26fvrTn37t+/KiD6GsrCxdddVV2rhxY5/rN27cqDlz5nha1cCLx+P68MMPVVpa6nspA6KyslKxWKzPfu3p6VFdXd2Q3q+S1NzcrIaGhkG1b40xWrJkiV5++WVt2rRJlZWVfW4fKvvzq7bzTAbj/jwTY4zi8fjXvy/7/a0OA2DdunUmFAqZ5557znzwwQdm6dKlJi8vzxw8eND30vrNgw8+aDZv3mw+/vhjs23bNnPzzTebSCQyqLexra3N7Nq1y+zatctIMk899ZTZtWuX+eSTT4wxxjz++OMmGo2al19+2ezdu9fccccdprS01LS2tnpeuZtzbWdbW5t58MEHzdatW019fb155513zOzZs83o0aMH1Xb+8Ic/NNFo1GzevNkcOXIkfens7EzXDIX9+VXbOVT25/Lly82WLVtMfX292bNnj3nkkUdMRkaGefvtt40xX+++HBQhZIwxf//3f28qKipMVlaWufLKK/u8ZXIoWLhwoSktLTWhUMiUlZWZBQsWmH379vle1gV55513jKTTLosWLTLG/OFtvY8++qiJxWImHA6b66+/3uzdu9fvos/Dubazs7PTVFVVmVGjRplQKGTGjh1rFi1aZA4dOuR72U7OtH2SzJo1a9I1Q2F/ftV2DpX9effdd6efT0eNGmW+9a1vpQPImK93X/J5QgAAby7614QAAEMXIQQA8IYQAgB4QwgBALwhhAAA3hBCAABvCCEAgDeEEADAG0IIAOANIQQA8IYQAgB48/8AyQd+82n94acAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Pick a random example from the training set\n",
    "classes = train.dataset.classes\n",
    "selection = random.randrange(len(train)-1)\n",
    "image, label = train[selection]\n",
    "\n",
    "show_cifar_img(image, label, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        ######################\n",
    "        # Convolutional layers\n",
    "        # (pattern augmentation)\n",
    "        ######################\n",
    "\n",
    "        # Conv2D is a convolutional layer that applies a 2D convolution over an input signal.\n",
    "        # TODO: set the in_channels, out_channels, kernel_size, stride, and padding parameters\n",
    "        self.conv1 = nn.Conv2d(..., ..., kernel_size=..., stride=..., padding=...)\n",
    "\n",
    "        # TODO: add more layers such as BatchNorm2d, MaxPool2d, and Conv2d\n",
    "\n",
    "        ######################\n",
    "        # Fully connected layers\n",
    "        # (pattern recognition)\n",
    "        ######################\n",
    "\n",
    "        # \"Flatten\" converts a 2D matrix to a vector to be able to feed it to the fully\n",
    "        # connected layers for classification. Should be between the convolutional and linear\n",
    "        # layers (as the convolutional layers output 3D tensors, and the linear layers expect 1D vectors)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # Linear is a fully connected layer that applies a linear transformation to the incoming data\n",
    "        # TODO: set the in_features and out_features parameters\n",
    "        self.fc1 = nn.Linear(..., ...)\n",
    "\n",
    "        # TODO: add more layers such as ReLU, Dropout, and Linear\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        ######################\n",
    "        # Convolutional forward pass\n",
    "        ######################\n",
    "        \n",
    "        # TODO: use the layers defined in the __init__ method to build the forward pass\n",
    "\n",
    "        x = self.flatten(x) # flatten the tensor (convert 2D matrix to a vector)\n",
    "\n",
    "        ######################\n",
    "        # Fully connected forward pass\n",
    "        ######################\n",
    "\n",
    "        # TODO: use the layers defined in the __init__ method to build the forward pass\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define relevant variables for the ML task\n",
    "LEARNING_RATE = ... # TODO: Set the learning rate\n",
    "WEIGHT_DECAY = ... # TODO: Set the weight decay (i.e. L2 regularization)\n",
    "NUM_EPOCHS = ... # TODO: Set the number of epochs to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for %: 'ellipsis' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mCNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# FYI; Sends the model to the GPU if set earlier\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Set Loss function with criterion\u001b[39;00m\n\u001b[1;32m      4\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "Cell \u001b[0;32mIn[13], line 12\u001b[0m, in \u001b[0;36mCNN.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28msuper\u001b[39m(CNN, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m######################\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Convolutional layers\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# (pattern augmentation)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Conv2D is a convolutional layer that applies a 2D convolution over an input signal.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# TODO: set the in_channels, out_channels, kernel_size, stride, and padding parameters\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1 \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# TODO: add more layers such as BatchNorm2d, MaxPool2d, and Conv2d\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m######################\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# connected layers for classification. Should be between the convolutional and linear\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# layers (as the convolutional layers output 3D tensors, and the linear layers expect 1D vectors)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mFlatten()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/aiml25-ma1/lib/python3.13/site-packages/torch/nn/modules/conv.py:521\u001b[0m, in \u001b[0;36mConv2d.__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[1;32m    519\u001b[0m padding_ \u001b[38;5;241m=\u001b[39m padding \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(padding, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m _pair(padding)\n\u001b[1;32m    520\u001b[0m dilation_ \u001b[38;5;241m=\u001b[39m _pair(dilation)\n\u001b[0;32m--> 521\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel_size_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdilation_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_pair\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/aiml25-ma1/lib/python3.13/site-packages/torch/nn/modules/conv.py:106\u001b[0m, in \u001b[0;36m_ConvNd.__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m groups \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroups must be a positive integer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43min_channels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_channels must be divisible by groups\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out_channels \u001b[38;5;241m%\u001b[39m groups \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for %: 'ellipsis' and 'int'"
     ]
    }
   ],
   "source": [
    "model = CNN().to(device) # FYI; Sends the model to the GPU if set earlier\n",
    "\n",
    "# Set Loss function with criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Set optimizer with optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE, weight_decay = WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model, history \u001b[38;5;241m=\u001b[39m fit(\n\u001b[0;32m----> 2\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m,\n\u001b[1;32m      3\u001b[0m     train_loader \u001b[38;5;241m=\u001b[39m train_loader,\n\u001b[1;32m      4\u001b[0m     val_loader \u001b[38;5;241m=\u001b[39m val_loader,\n\u001b[1;32m      5\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m optimizer,\n\u001b[1;32m      6\u001b[0m     criterion \u001b[38;5;241m=\u001b[39m criterion,\n\u001b[1;32m      7\u001b[0m     num_epochs \u001b[38;5;241m=\u001b[39m NUM_EPOCHS,\n\u001b[1;32m      8\u001b[0m     device \u001b[38;5;241m=\u001b[39m device,\n\u001b[1;32m      9\u001b[0m     flatten \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# don't flatten input tensors to 1D before passing to model. Convolutonal layers expect nD.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model, history = fit(\n",
    "    model = model,\n",
    "    train_loader = train_loader,\n",
    "    val_loader = val_loader,\n",
    "    optimizer = optimizer,\n",
    "    criterion = criterion,\n",
    "    num_epochs = NUM_EPOCHS,\n",
    "    device = device,\n",
    "    flatten = False  # don't flatten input tensors to 1D before passing to model. Convolutonal layers expect nD.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Plot loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plot_training_history(\u001b[43mhistory\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate on training data, validation data and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m evaluate(\n\u001b[0;32m----> 2\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m,\n\u001b[1;32m      3\u001b[0m     data_loader \u001b[38;5;241m=\u001b[39m train_loader,  \u001b[38;5;66;03m# evaluate on training data\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     device \u001b[38;5;241m=\u001b[39m device,\n\u001b[1;32m      5\u001b[0m     criterion \u001b[38;5;241m=\u001b[39m criterion,\n\u001b[1;32m      6\u001b[0m     flatten \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# don't flatten input tensors to 1D. Convolutonal layers expect nD.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "evaluate(\n",
    "    model = model,\n",
    "    data_loader = train_loader,  # evaluate on training data\n",
    "    device = device,\n",
    "    criterion = criterion,\n",
    "    flatten = False  # don't flatten input tensors to 1D. Convolutonal layers expect nD.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m image, label \u001b[38;5;241m=\u001b[39m test[index]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Prepare the image for prediction\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# Set the model to evaluation mode\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      8\u001b[0m     input_image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)                     \u001b[38;5;66;03m# Add batch dimension and move to device\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Get a single image and its label from the test dataset\n",
    "index = np.random.randint(len(test))  # Randomly select an index\n",
    "image, label = test[index]\n",
    "\n",
    "# Prepare the image for prediction\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    input_image = image.unsqueeze(0).to(device)                     # Add batch dimension and move to device\n",
    "    outputs = model(input_image)                                    # Get model predictions\n",
    "    probabilities = torch.nn.functional.softmax(outputs[0], dim=0)  # Apply softmax to get probabilities\n",
    "\n",
    "plot_probabilities(\n",
    "    image = image,\n",
    "    label = label,\n",
    "    probabilities = probabilities, \n",
    "    classes = test.classes,\n",
    "    n = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m evaluate(\n\u001b[0;32m----> 2\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m,\n\u001b[1;32m      3\u001b[0m     data_loader \u001b[38;5;241m=\u001b[39m val_loader,  \u001b[38;5;66;03m# evaluate on validation data\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     device \u001b[38;5;241m=\u001b[39m device,\n\u001b[1;32m      5\u001b[0m     criterion \u001b[38;5;241m=\u001b[39m criterion,\n\u001b[1;32m      6\u001b[0m     flatten \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "evaluate(\n",
    "    model = model,\n",
    "    data_loader = val_loader,  # evaluate on validation data\n",
    "    device = device,\n",
    "    criterion = criterion,\n",
    "    flatten = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m evaluate(\n\u001b[0;32m----> 2\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m,\n\u001b[1;32m      3\u001b[0m     data_loader \u001b[38;5;241m=\u001b[39m test_loader,  \u001b[38;5;66;03m# evaluate on testing data\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     device \u001b[38;5;241m=\u001b[39m device,\n\u001b[1;32m      5\u001b[0m     criterion \u001b[38;5;241m=\u001b[39m criterion,\n\u001b[1;32m      6\u001b[0m     flatten \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "evaluate(\n",
    "    model = model,\n",
    "    data_loader = test_loader,  # evaluate on testing data\n",
    "    device = device,\n",
    "    criterion = criterion,\n",
    "    flatten = False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiml25-ma1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
